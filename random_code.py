import jobadcollector.test as test
import datetime
import sqlite3 

#run tests
dbtests = test.DBTests()
dbtests.create_db_class_test()
dbtests.connect_db_test()
dbtests.store_ads_test()
dbtests.update_ads_test()
dbtests.get_classified_ads_test()


#test classification
import jobadcollector

my_search_terms = [
    'Analyytikko',
    'Analyst',
    'Physics',
    'Fysiikka',
    'Fyysikko',
    'Science',
    'M.Sc',
    'FM',
    'Entry',
    'First',
    'Graduate',
    'Associate',
    'Matlab',
    'Tohtorikoulutettava',
    'Doctoral',
    'Materials',
    'Materiaali',
    'Diplomi',
    'Machine learning',
    'Koneoppiminen']

#js = jobadcollector.JobAdCollector(my_search_terms, "tmp.db", classification=True)
#js.det_lang_store_ads(datetime.datetime.strptime("20-10-2015", "%d-%m-%Y"), 
#                      datetime.datetime.strptime("01-11-2015", "%d-%m-%Y"))
#RFC = js.train_model(language="English")
#js.recomm_store_ads(RFC, "English", 
#                      datetime.datetime.strptime("20-10-2015", "%d-%m-%Y"), 
##                      datetime.datetime.strptime("01-11-2015", "%d-%m-%Y"))
#RFC = js.train_model(language="Finnish")
#js.recomm_store_ads(RFC, "Finnish", 
#                      datetime.datetime.strptime("20-10-2015", "%d-%m-%Y"), 
#                      datetime.datetime.strptime("01-11-2015", "%d-%m-%Y"))


#js.output_results(datetime.datetime.strptime("20-10-2015", "%d-%m-%Y"), 
#                  datetime.datetime.strptime("01-11-2015", "%d-%m-%Y"),
#                  "test.html", "html")

conn = sqlite3.connect("tmp.db")
c = conn.cursor()

db = c.execute("""SELECT * FROM JobEntries WHERE searchterm LIKE '%0%'""").fetchall()

for d in db:
    print(str(d).encode("unicode_escape"))
    c.execute("""UPDATE JobEntries SET searchterm = "Machine learning" WHERE id=?""", (d[2],))

conn.commit()


class OldDuunitoriParser(HTMLParser):
    """
    HTML parser for Duunitori.fi job searches. Saves results in an object dictionary 
    as {id: [title, url]}, where id is taken from the webpage.
    """
    job = 0
    a_data = 0
    title_added = 0
    job_list = 0
    job_ads = []
    id = ""
    title = ""
    url = "" 
    additional = ""
    
    def __init__(self):
        super(DuunitoriParser, self).__init__(convert_charrefs=True)

    def save_job_ad(self, id, title, url, additional):
        """
        Saves job ad parsed from html in instance dictionary. 

        id          - Unique id for job ad, either from the website or 
                      generated by the parser.
        title       - Job title.
        url         - URL to the job ad.
        description - Any additional information regarding the job ad.        
        """
        self.job_ads.append({"id" : id,
                             "title" : re.sub("\s{2,}", " ", title.strip()), 
                             "url" : url, 
                             "description" : re.sub("\s{2,}", " ", additional.strip())})

    def get_job_ads(self):
        """
        Returns the object's dictionary of job entries.
        """
        return self.job_ads

    def handle_starttag(self, tag, attrs):
        if (tag == "div" and ('id', 'jobentry-list') in attrs):
            self.job_list = 1
        elif (self.job_list == 1 and tag == "div" and 
              ('id', 'bottom-pagination') in attrs):
            self.job_list = 0
        elif (self.job_list == 1 and tag == "article" and 
              ('class', 'jobentry-item') in attrs):
            self.job = 1
            for entry in attrs:
                if entry[0] == 'data-id':
                    self.id = entry[1]
        elif (self.job_list == 1 and tag == "a" and self.job == 1 and 
              ('class', 'jobentry-link') in attrs):
            self.a_data = 1
            for entry in attrs:
                if entry[0] == "href":
                    self.url = "http://www.duunitori.fi/" + entry[1].strip()

    def handle_endtag(self, tag):
        if (self.job_list == 1 and tag == "a" and self.a_data == 1):
            self.title_added = 1
            self.a_data = 0 
            self.job = 0
        elif(self.job_list == 1 and self.title_added == 1 and tag == "article"):
            self.save_job_ad(self.id, self.title, self.url, self.additional)
            self.title_added = 0
            self.title = ""
            self.id = ""
            self.url = ""
            self.additional = ""
            
    def handle_data(self, data):
        if (self.job_list == 1 and self.job == 1 and self.a_data == 1):
            self.title = self.title + " " + data.strip()
        elif(self.job_list == 1 and self.title_added == 1):
            self.additional = self.additional + " " + data.strip()

    def parse_URL(self, URL):
        url_req = urllib.request.urlopen(URL)
        encoding = url_req.headers.get_content_charset()
        url_req_text = re.sub("\s+", " ", url_req.read().decode(encoding))
        url_req_text = re.sub("(&nbsp;)+", " ", url_req_text)
        self.feed(url_req_text)